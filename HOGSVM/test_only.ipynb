{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage.feature import hog\n",
    "from joblib import load,dump\n",
    "import os\n",
    "from tqdm import tqdm,trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters: Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These must match those of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "patchsize_yx = (128,128)\n",
    "orientations=8\n",
    "pixels_per_cell=(16, 16)\n",
    "cells_per_block=(2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These can be changed to improve sliding window/non-max suppression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sws_percent = [22,25]\n",
    "overlap_thresh = 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Running The Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sliding Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(image, stepSize, windowSize,returnSize = patchsize_yx):# image is the input, step size is the no.of pixels needed to skip and windowSize is the size of the actual window\n",
    "    # slide a window across the image\n",
    "    for y1 in range(0, image.shape[1] - windowSize[1], stepSize):# this line and the line below actually defines the sliding part and loops over the x and y coordinates\n",
    "        for x1 in range(0, image.shape[0] - windowSize[0], stepSize):\n",
    "            # yield the current window\n",
    "            #yield (x, y, image[y: y + windowSize[1], x:x + windowSize[0]])\n",
    "            y2 = y1 + windowSize[0]\n",
    "            x2 = x1 + windowSize[1]\n",
    "            patch = image[y1: y2, x1: x2]\n",
    "            if patch.shape[0:2] == windowSize:\n",
    "                return_patch = cv2.resize(patch,returnSize)\n",
    "                yield (x1, y1,x2, y2, return_patch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Area funcs for non-max suppression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def area(rect):\n",
    "    x1,y1,x2,y2 = rect\n",
    "    return (x2-x1+1)*(y2-y1+1)\n",
    "\n",
    "def area_intersect(rect1,rect2):\n",
    "    x1,y1,x2,y2 = rect1\n",
    "    x3,y3,x4,y4 = rect2\n",
    "    if x2 < x3:\n",
    "        return 0\n",
    "    if x4 < x1:\n",
    "        return 0\n",
    "    if y2 < y3:\n",
    "        return 0\n",
    "    if y4 < y1:\n",
    "        return 0\n",
    "    X1 = max(x1,x3)\n",
    "    X2 = min(x2,x4)\n",
    "    Y1 = max(y1,y3)\n",
    "    Y2 = min(y2,y4)\n",
    "    return area((X1,Y1,X2,Y2))\n",
    "\n",
    "def area_union(rect1,rect2):\n",
    "    return area(rect1) + area(rect2) - area_intersect(rect1,rect2)\n",
    "\n",
    "def union_to_intersect(rect1,rect2):\n",
    "    return area_intersect(rect1,rect2)/area_union(rect1,rect2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Big Boy for counting Among Us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def how_many_amogus(model,test_img,ret_labeled_img = False ,sws_percent= sws_percent, overlap_thresh= overlap_thresh):\n",
    "    sws_array = np.array(sws_percent)\n",
    "    imh,imw,nchannel = test_img.shape\n",
    "    short_side = min(imh,imw)\n",
    "    sliding_window_sizes = np.round(sws_array*short_side/100).astype(int)\n",
    "    step_size = np.ceil(sliding_window_sizes/8).astype(int)\n",
    "\n",
    "\n",
    "    patches = []\n",
    "    for i in range(len(sws_percent)):\n",
    "        tup = (sliding_window_sizes[i],sliding_window_sizes[i])\n",
    "        patches += list(sliding_window(test_img,step_size[i],tup))\n",
    "\n",
    "    patches_hog = np.array([hog(patch, orientations=orientations, pixels_per_cell=pixels_per_cell,cells_per_block=cells_per_block, visualize=False, channel_axis=-1)\\\n",
    "        for _,_,_,_,patch in patches])\n",
    "\n",
    "    labels = model.predict(patches_hog)\n",
    "    confidence_in_class1 = model.predict_proba(patches_hog)\n",
    "\n",
    "    labels_confidence = []\n",
    "    for i,patch in enumerate(patches):\n",
    "        if labels[i] == 1:\n",
    "            x1,y1,x2,y2,_ = patch\n",
    "            labels_confidence.append((confidence_in_class1[i,1],x1,y1,x2,y2))\n",
    "\n",
    "    labels_confidence.sort(reverse=True)\n",
    "    true_amogus = []\n",
    "    for element in labels_confidence:\n",
    "        conf = element[0]\n",
    "        rect = element[1:]\n",
    "        not_duplicate = True\n",
    "        for _,true_rect in true_amogus:\n",
    "\n",
    "            if union_to_intersect(rect,true_rect) > overlap_thresh:\n",
    "                not_duplicate = False\n",
    "                break\n",
    "        if not_duplicate:\n",
    "            true_amogus.append((conf,rect))\n",
    "\n",
    "    if ret_labeled_img:\n",
    "        image = test_img.copy()\n",
    "        for conf,rect in true_amogus:\n",
    "            x1,y1,x2,y2, = rect\n",
    "            image = cv2.rectangle(image,(x1,y1),(x2,y2),(255,0,0),4)\n",
    "            \n",
    "        for conf,rect in true_amogus:\n",
    "            x1,y1,x2,y2, = rect\n",
    "            image = cv2.putText(image,str(round(conf,5)),(x1,y1),cv2.FONT_HERSHEY_SIMPLEX,0.6,(0,192,192),2,cv2.LINE_AA)\n",
    "        return len(true_amogus),image\n",
    "    else:\n",
    "        return len(true_amogus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evalution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAE(y_true,y_pred):\n",
    "     y_true = np.array(y_true)\n",
    "     y_pred = np.array(y_pred)\n",
    "     return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "def RMSE(y_true,y_pred):\n",
    "     y_true = np.array(y_true)\n",
    "     y_pred = np.array(y_pred)\n",
    "     return np.sqrt(np.mean((y_true - y_pred)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters: Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_dir = \"Test_Pic/set3\"\n",
    "answer_file = 'count.txt'\n",
    "model_path = 'Model/model.joblib'\n",
    "numofpic = 1505"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model and Initialize some to-be-used lists "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load(model_path)\n",
    "result = []\n",
    "answers = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop for image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in trange(numofpic):\n",
    "for i in trange(20):\n",
    "    test_img = cv2.imread(f'{test_img_dir}/img{i}.jpg')\n",
    "    test_img = cv2.cvtColor(test_img,cv2.COLOR_BGR2RGB)\n",
    "    x = how_many_amogus(model,test_img)\n",
    "    result.append(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop for true values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{test_img_dir}/{answer_file}','r') as file:\n",
    "    for line in file:\n",
    "        i, a = line.split()\n",
    "        answers.append(int(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MAE(result,answers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(RMSE(result,answers))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "afb734500600fd355917ca529030176ea0ca205570884b88f2f6f7d791fd3fbe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
